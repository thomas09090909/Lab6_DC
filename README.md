# Customer Churn Prediction using Spark ML on Amazon EMRProject
 OverviewThis project implements an end-to-end Spark ML Pipeline for predicting bank customer churn. The solution is deployed on an Amazon EMR cluster, leveraging distributed computing to process features and train models at scale.DatasetSource: Bank Customer Churn Dataset (Kaggle)URL: https://www.kaggle.com/datasets/shrutimechlearn/churn-modellingSize: 10,000 recordsTarget Variable: Exited (0 = Stayed, 1 = Churned)Features: CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary.ML Pipeline ArchitectureThe project follows the Spark ML Pipeline abstraction with the following stages:StringIndexer: Converts categorical strings (Gender, Geography) into numerical indices.OneHotEncoder: Transforms indices into binary vectors.VectorAssembler: Combines all numerical and encoded features into a single feature vector.StandardScaler: Normalizes features to have zero mean and unit variance.Classifier: Either Logistic Regression (Baseline) or Random Forest (Experiment).Setup & Execution1. Prepare HDFSUpload the dataset to the Hadoop Distributed File System:Bashhdfs dfs -mkdir -p /user/hadoop/churn_input
hdfs dfs -put Churn_Modelling.csv /user/hadoop/churn_input/
2. Run Baseline (Logistic Regression)Bashspark-submit --master yarn --deploy-mode client churn_pipeline.py
3. Run Experiment (Random Forest)Bashspark-submit --master yarn --deploy-mode client churn_pipeline_rf.py
Experiment Results (Model Comparison)I compared the baseline Logistic Regression model against a Random Forest Classifier.MetricRandom Forest ResultAccuracy0.8412Precision0.8359Recall0.8412F1 Score0.8109Observation: The Random Forest model achieved high accuracy on the EMR cluster. Distributed execution allowed for parallel construction of trees across the worker nodes, significantly improving processing time compared to a single-node execution.MonitoringExecution was monitored using the YARN ResourceManager UI (Port 8088).Executors: 2 Core Nodes (m4.large)Task Distribution: Managed by YARN for optimized data locality.
